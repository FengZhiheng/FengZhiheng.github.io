<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"fengzhiheng.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="在YouTube上学习李宏毅的机器学习课程。">
<meta property="og:type" content="article">
<meta property="og:title" content="MachineLearning-机器学习-课程笔记">
<meta property="og:url" content="https://fengzhiheng.github.io/2021/07/29/%E7%AC%94%E8%AE%B0(%E6%9D%8E%E5%AE%8F%E6%AF%85)-MachineLearning/index.html">
<meta property="og:site_name" content="Feng Zhiheng&#39;s Blog">
<meta property="og:description" content="在YouTube上学习李宏毅的机器学习课程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1n5s040o4j21590u00wr.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1n5ubvxnoj218y0u0tdj.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1n67kx67sj218a0n276m.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qonlxb47j21840mfdj2.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qordlnf3j216s0htmyt.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qov0gictj213e0odq5f.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qowchjwzj216p0enjtg.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qp3v24mlj212h0o7die.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qp7h07rbj21bi0p2q64.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpe9y33kj21d20rm42u.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpjb7jb3j21c10qywi7.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpp1t2ppj21bt0qvaf2.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpqsc1yoj214u0lyta9.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpr2gik8j215w0ovmz6.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpvyei1dj21910pyaej.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpx61bedj215d0otdiy.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qqy0181lj21770pdwhm.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qr3o08ivj217n0oawg4.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qr5kalcdj210v0pdacy.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qr9yuafoj218s0pon0f.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qrj3reeyj219y0ptgp5.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qru0be7ij219m0phdiv.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qscufurtj21780pa0vp.jpg">
<meta property="og:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1qsqa3hb5j215v0p2775.jpg">
<meta property="article:published_time" content="2021-07-29T03:46:24.000Z">
<meta property="article:modified_time" content="2025-09-14T13:57:40.006Z">
<meta property="article:author" content="Feng Zhiheng">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ww3.sinaimg.cn/large/a80ede49gy1h1n5s040o4j21590u00wr.jpg">

<link rel="canonical" href="https://fengzhiheng.github.io/2021/07/29/%E7%AC%94%E8%AE%B0(%E6%9D%8E%E5%AE%8F%E6%AF%85)-MachineLearning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>MachineLearning-机器学习-课程笔记 | Feng Zhiheng's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Feng Zhiheng's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">      
      

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">      
      

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">      
      

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://fengzhiheng.github.io/2021/07/29/%E7%AC%94%E8%AE%B0(%E6%9D%8E%E5%AE%8F%E6%AF%85)-MachineLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Feng Zhiheng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Feng Zhiheng's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MachineLearning-机器学习-课程笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-29 11:46:24" itemprop="dateCreated datePublished" datetime="2021-07-29T11:46:24+08:00">2021-07-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-09-14 21:57:40" itemprop="dateModified" datetime="2025-09-14T21:57:40+08:00">2025-09-14</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>18k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>16 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>在YouTube上学习李宏毅的机器学习课程。</p>
<span id="more"></span>
<p>1、学习这门课的出发点。</p>
<p>1）为以后找到一份跟机器学习算法相关的工作。</p>
<p>2）在TN神经痛课题上，运用机器学习相关技术，挖掘数据背后的规律。</p>
<p>​    第一个需要挖掘的规律是，利用颌面部表情，挖掘疼痛强度之间的关系；</p>
<p>​    第二个需要挖掘的规律是，利用小鼠的神经元，挖掘和疼痛强度之间的关系；</p>
<p>2、希望达到的水平</p>
<p>1）理论层面，对于如何构建机器学习模型有系统的认知。（掌握几个主流的Machine Learning框架，Support Vector Machine、CNN、RNN、LSTM）；</p>
<p>2）实践层面，熟悉tensorflow框架，能够搭建、修改深度学习模型；</p>
<p>3）像使用PPT一样，使用深度学习模型。</p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=CXgbekl66jc&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;ab_channel=Hung-yiLee">https://www.youtube.com/watch?v=CXgbekl66jc&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;ab_channel=Hung-yiLee</a></p>
<p>机器学习2017</p>
<p>课程网站：<a target="_blank" rel="noopener" href="http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html">http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML17_2.html</a></p>
<p>开源笔记网站：<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/leeml-notes/#/">https://datawhalechina.github.io/leeml-notes/#/</a></p>
<p>3、开始课程吧~</p>
<h4 id="【機器學習2021】預測本頻道觀看人數-上-機器學習基本概念簡介"><a href="#【機器學習2021】預測本頻道觀看人數-上-機器學習基本概念簡介" class="headerlink" title="【機器學習2021】預測本頻道觀看人數 (上) - 機器學習基本概念簡介"></a>【機器學習2021】預測本頻道觀看人數 (上) - 機器學習基本概念簡介</h4><p><a target="_blank" rel="noopener" href="https://youtu.be/Ye018rCVvOo?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=930">https://youtu.be/Ye018rCVvOo?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=930</a></p>
<p>机器学习能够解决的问题：回归（预测明天股票的价格）、分类（n个类别）、Structured Learning（让机器产生一个有结构的东西，比如绘制一张图、写一段话）。</p>
<ul>
<li><p>机器学习  training 的三个关键步骤：</p>
<p>第一步：定义模型model，（说白了，就是带有参数的函数）。$y = b + w x$​​<br>第二步：定义模型的评价函数Loss Function（Loss是一个跟参数相关的函数）。选择Loss函数$L$​</p>
<p>​    loss is a function of parameters。L(b,w)，模型的输出和真实label之间的差异。</p>
<ul>
<li>Mean Absolute Error  （平均绝对误差）、Mean Square Erroe（平均均方误差）、交叉熵（Cross-Entropy）  </li>
<li>error surface（通过调整不同参数，绘制出的loss结果）；</li>
</ul>
<p>第三步：优化（optimization） $w^<em>, b^</em> = arg\ \mathop{min}\limits_{w,b}\ L$​​</p>
<p>​    使用梯度下降法，找到使得$L$​最小的$w^*$​ 和$</p>
</li>
<li><p>Model  Bias:来自model本身的限制</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=115">https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=115</a></p>
<ul>
<li>Sigmoid函数，多个sigmoid函数叠加起来可以逼近任何函数。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=544">https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=544</a></p>
<script type="math/tex; mode=display">
y = c\frac{1}{1+e^{-(b+wx)}} = c\ sigmoid(b+wx)</script><h4 id="【機器學習2021】預測本頻道觀看人數-下-深度學習基本概念簡介"><a href="#【機器學習2021】預測本頻道觀看人數-下-深度學習基本概念簡介" class="headerlink" title="【機器學習2021】預測本頻道觀看人數 (下) - 深度學習基本概念簡介"></a>【機器學習2021】預測本頻道觀看人數 (下) - 深度學習基本概念簡介</h4><ul>
<li>课程笔记：关于epoch和batch size的区别：</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2378">https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2378</a></p>
<ul>
<li>ReLu函数</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2520">https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2520</a></p>
<p>两个ReLU函数加起来，等于一个soft sigmoid函数</p>
<p>机器学习中的Activation Function</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2622">https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2622</a></p>
<ul>
<li>为什么深度学习不横向发展，而要纵向发展？（为啥不搞fat netkwork ，而叫deep network）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3194">https://youtu.be/bHcJCp2Fyxs?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3194</a></p>
<p>层数越深，越容易过拟合。</p>
<p>机器学习三步走：</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=248">https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=248</a></p>
<p>1、function with unknow；$y = f_\theta(x)$</p>
<p>2、define loss function；$L(\theta)$</p>
<p>3、optimization；$\theta^* = arg\ \mathop{min}\limits_{\theta}\ L$​</p>
<p>4、使用模型进行预测 $y<em>{predict} = f</em>{\theta^*}(x)$</p>
<ul>
<li>如何做一个调参师：（调整模型结构、调整loss function、通过做batch normolization使得error surface更加平坦、做数据增强、增加训练集的样本）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=312">https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=312</a></p>
<ul>
<li>什么时候才需要增加训练集的样本？</li>
</ul>
<p>当你的模型出现Overfitting的时候才需要增加训练样本。overfitting的症状是，模型在train dataset上的loss非常低，但是在test dataset上的loss非常高。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1285">https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1285</a></p>
<p>在无法收集资料的前提下，该如何增加模型在test dataset上表现呢？答案有：数据增强（data  augmentation）、对模型施加约束（更少的参数、参数共享、less feature、early  stopping、regularization、drop）</p>
<ul>
<li>为什么要把training set分成两份（training set和validation set）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2466">https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2466</a></p>
<ul>
<li>什么是mismatch（训练样本的分布和预测样本的分布不一样）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2933">https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2933</a></p>
<ul>
<li>如何知道是不是mismatch?(要理解训练资料和测试资料的产生方式)</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3061">https://youtu.be/WeHM2xpYQpw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3061</a></p>
<ul>
<li>模型都不知道，函数都不清楚，如何计算梯度？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/leeml-notes/#/chapter3/chapter3">https://datawhalechina.github.io/leeml-notes/#/chapter3/chapter3</a></p>
<h4 id="【機器學習2021】類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point"><a href="#【機器學習2021】類神經網路訓練不起來怎麼辦-一-：-局部最小值-local-minima-與鞍點-saddle-point" class="headerlink" title="【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)"></a>【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)</h4><ul>
<li><p>梯度为零的点包括local minimun、saddle point —-&gt;统称为critical point</p>
</li>
<li><p>海森矩阵的eign value有正有负，代表saddle point</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/QW6uINn7uGk?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1158">https://youtu.be/QW6uINn7uGk?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1158</a></p>
<ul>
<li>用海森矩阵的方法逃离saddle point，是实际情况中并不常用。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/QW6uINn7uGk?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1514">https://youtu.be/QW6uINn7uGk?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1514</a></p>
<ul>
<li>关于优化函数的小结：</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/HYUXEeh3kwY?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2197">https://youtu.be/HYUXEeh3kwY?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2197</a></p>
<ul>
<li>能不能把error surface 给“炸”平？</li>
</ul>
<p>通过修改loss function 能够改变optimization的难度</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1165">https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1165</a></p>
<ul>
<li>分类问题的标签如何打？one-hot vector</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=214">https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=214</a></p>
<ul>
<li>什么是softmax function？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=365">https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=365</a></p>
<p>softmax的输入叫logit</p>
<ul>
<li>两个class的时候，sigmoid和softmax是一样的。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=653">https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=653</a></p>
<ul>
<li>classficition的loss（MSE、和cross entropy）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=723">https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=723</a></p>
<ul>
<li>最小化 cross-entropy 和最大化 likelyhood是一模一样的东西。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=723">https://youtu.be/O2VkP8dJ5FE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=723</a></p>
<ul>
<li><p>softmax和cross entropy在pytorch中被绑定在一次。</p>
</li>
<li><p>什么是batch normalization</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/BABPWOkSbLE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1047">https://youtu.be/BABPWOkSbLE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1047</a></p>
<ul>
<li>batch normaization能够是的error face变得更加光滑</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/BABPWOkSbLE?t=1725">https://youtu.be/BABPWOkSbLE?t=1725</a></p>
<ul>
<li>各种各样的normalization方法：</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/BABPWOkSbLE?t=1725">https://youtu.be/BABPWOkSbLE?t=1725</a></p>
<p>network的设计，有什么想法。</p>
<h4 id="【機器學習2021】卷積神經網路-Convolutional-Neural-Networks-CNN"><a href="#【機器學習2021】卷積神經網路-Convolutional-Neural-Networks-CNN" class="headerlink" title="【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)"></a>【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)</h4><ul>
<li><p>如何强制限制模型的结构（比如只让模型关注一小部分，一个局部，通过设置Receptive Field）</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1624">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1624</a></p>
</li>
<li><p>通过参数共享（parameter sharing）进一步限制模型的弹性</p>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1644">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1644</a></p>
<ul>
<li>什么是Feature Map(假设你有一张图片100×100×3的图片，然后有64个3×3×3(chanel)的filter，经过一个卷积层之后得到98×98*64的Feature Map，然后这个Feature Map，你可以再看成是一张图像，只不过这张图片的chanel不是3了，是64个channel)。当然convolution layer可以叠很多层，那么第二层里的filter呢，它的大小就是3×3×64了</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2052">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2052</a></p>
<ul>
<li>换一种方式理解CNN，convolution 由各种各样的filter构成，每个filter的功能是从图像中抓feature</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1836">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1836</a></p>
<ul>
<li>pooling存在的目的是为了减少运算量，一般做两层卷积，做一次pooling。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2611">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2611</a></p>
<p>但是有些场景就不能用pooling，比如AlphaGo里边，就没有pooling，因为pooling之后，把某些列删掉后，对围棋来说就没道理了。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3164">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3164</a></p>
<ul>
<li>Flatten是干什么的？把排成矩阵的tensor，拉直成一个向量。然后丢给fully connection layer，再过一个softmax，最终得到输出。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2648">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2648</a></p>
<ul>
<li>将CNN运用到语音识别、自然语言处理上时，需要注意的点有：receptive field设计、参数共享设计等注意事项。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3207">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3207</a></p>
<ul>
<li>CNN不能处理图片放大缩小的问题；（训练前，做图像增强）</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3310">https://youtu.be/OP5HcXJg2Aw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=3310</a></p>
<h4 id="【機器學習2021】自注意力機制-Self-attention-上"><a href="#【機器學習2021】自注意力機制-Self-attention-上" class="headerlink" title="【機器學習2021】自注意力機制 (Self-attention) (上)"></a>【機器學習2021】自注意力機制 (Self-attention) (上)</h4><ul>
<li>Self-attention神经网络解决什么问题？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=14">https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=14</a></p>
<p>现在的网络，<strong>输入是一个向量</strong>，输出是一个标量或者类别。但是很多任务中，输入是一堆向量，而且输入向量的长度还会发生改变。该怎么办？（比如输入一个句子，句子的长短不是固定的，而且每个单词之间是有联系的。要求输出句子中每个单词的词性，该怎么弄？one-hot的编码格式，使得单词之间的关联性消失）。但是需要注意的是，这类任务中，输入和输出的长度是一样的。（输入几个向量，输出几个向量）</p>
<p>还有一种可能，输入是多个向量，但只需要输出一个label。（比如语句分析，给一个句子，输出是积极的评价，还是消极的评价）</p>
<p>还有一个可能，输入是多个向量。输出也是多个向量。但是输出的个数不是固定的。这类任务是seq2seq任务。</p>
<p>以上总结了三种任务：N对N，N对1，N对M（M是机器决定的）；</p>
<p>本节课的内容，针对的是第一类问题:N对N；（sequence labeling任务）</p>
<p>解决“<strong>I saw a saw</strong>”这样的问题。如何解决？</p>
<p>1、加窗（考虑前后5个vector）；</p>
<p>2、考虑整个sequence；如何做到呢？就需要用到self   attention技术了；self -attention 会吃掉整个sequence的信息；Input结果Vector，就输出几个Vector；</p>
<ul>
<li>如何更好地考虑整个input的信息？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=966">https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=966</a></p>
<p>1、第一步，根据$a^1$找出整个sequence中哪些部分是重要的，这部分对于判断$a^1$的label是重要的；</p>
<p>2、sequence中，每向量跟$a^1$关联的程度用一个数值表示$\alpha$​</p>
<p>3、self-attention如何决定两个向量之间的关联性呢？（<a target="_blank" rel="noopener" href="https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1298），有两种做法：第一种是dot-product。引入了两个矩阵$W^q$​和$W^k$​​​">https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1298），有两种做法：第一种是dot-product。引入了两个矩阵$W^q$​和$W^k$​​​</a>  ​，还可以采用Additive的方法</p>
<p>4、$q^1 = W^q\ a^1$ (q是query的缩写)，其他的向量$k^2 = W^k a^2$（k是key的缩写），然后将$q^1$和$k^2$做内积，得到$\alpha<em>{1,2}$（1代表query是$a^1$提供的，2代表key是$k^2$提供的）,$\alpha</em>{1,2}$也叫做attention score。</p>
<p>5、$a^1$也会跟自己做相关性；</p>
<p>6、每一个$\alpha$都会经过一个soft-max（当然也可以经过ReLu函数） ，得到$\alpha’$</p>
<p>7、接下来要根据这些$\alpha$​（相关性）去抽取整个Sequence中重要的信息；此时引入$W^v$​矩阵，用$W^v$​和$a^i$​相乘得到$v^i$​，即$v^i = W^va^i$​ ；</p>
<p>8、每个$v^i$​和$\alpha’$​相乘，<strong>求和</strong>，最终得到输出label  ，$b^i$​​ ；</p>
<p>9、谁的attention最大，输出结果就受谁的影响最大；</p>
<hr>
<p>从矩阵惩罚的角度，重新讲一遍：</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=215">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=215</a></p>
<ul>
<li>self attention可以叠加很多层；</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1113">https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1113</a></p>
<ul>
<li>自监督层中的特征如何计算？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1691">https://youtu.be/hYdO9CscNes?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1691</a></p>
<ul>
<li>什么是模型的embedding? The embedding in machine learning or NLP is actually a technique mapping from words to vectors which you can do better analysis or relating。就是把单词变成向量的方法。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/53995/what-does-embedding-mean-in-machine-learning">https://datascience.stackexchange.com/questions/53995/what-does-embedding-mean-in-machine-learning</a></p>
<ul>
<li>多个模型如何级联到一起？ Model Ensemble</li>
</ul>
<h4 id="【機器學習2021】自注意力機制-Self-attention-下"><a href="#【機器學習2021】自注意力機制-Self-attention-下" class="headerlink" title="【機器學習2021】自注意力機制 (Self-attention) (下)"></a>【機器學習2021】自注意力機制 (Self-attention) (下)</h4><ul>
<li>做语音识别的时候，有个技术叫Truncated self-attention技术，它解决“如果输入样本L太大”这类问题。（解决如何避免内存爆炸的问题）；</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1664">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1664</a></p>
<ul>
<li>self attention 如何运用到图像处理上？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1686">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1686</a></p>
<p>讲CNN的时候，把一张图像看成一个很长的Vector；</p>
<p>现在把一张图像看成一个vector set;</p>
<ul>
<li>self attention和CNN有什么差异？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1828">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1828</a></p>
<p>（1）CNN可以看做是一种简化版的self-attention。因为CNN只考虑 receptive field里的信息。</p>
<p>（2）反过来说，self-attention是一种复杂化的CNN，在CNN中，只关注receptive field里的信息，receptive field的大小和尺寸是人设定了。而在self-attention中，这个receptive field是由机器自己学出来的。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1877">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1877</a></p>
<ul>
<li>self attention和RNN之间的差异：</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2122">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2122</a></p>
<p>1、RNN和Self Attention一样，都需要处理输入是一个Sequence的情况；</p>
<p>2、对于第一个input vector，RNN吃掉该Vector和一个memory；</p>
<p>3、对于第二个input vector，RNN吃掉该vector和上个RNN的输出；</p>
<p>4、以此类推；</p>
<p>—两者有什么不同呢？</p>
<p>1、对于self attention 来说，每个输出的vector都考虑了整个sequence；但是RNN输出的每个vector，只考虑了左边的vector，没有考虑右边的vector；</p>
<p>2、李宏毅老师举了一个例子，拿最右边的Output Vector来说，如果希望考虑第一个输入Vector该怎么办？在Self Attention中，只要第一个vector提供的key和query匹配就可以了，但是在RNN中，第一个Vector提供的信息得一直在memory中保留才行；</p>
<p>3、RNN在输出output的sequence时，没有办法并行；而self-attention可以并行；</p>
<ul>
<li>RNN相关的神经网络介绍：</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2423">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2423</a></p>
<p>RNN在之前的课程中已经介绍过，网址如下：</p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xCGidAeyS4M&amp;ab_channel=Hung-yiLee">https://www.youtube.com/watch?v=xCGidAeyS4M&amp;ab_channel=Hung-yiLee</a></p>
<p>一个背景问题：Slot Filling。引出来：我们希望我们的网络是有记忆力的；</p>
<p>在RNN中，网络会考虑输入序列的order；</p>
<p>在RNN中有多种不同的类型，</p>
<p>比如<br>（1）Elman Network：把hidden layer的值存起来，下个时刻再读出来<br>（2）Jordan Network：把整个网络的output存起来，<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1n5s040o4j21590u00wr.jpg" alt=""><br>（3）双向RNN  Bidirectional RNN  好处是，网络观测的范围比较广。<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1n5ubvxnoj218y0u0tdj.jpg" alt=""></p>
<p>以上属于基础版的RNN，还有升级版的RNN，比如LSTM，</p>
<p>LSTM中也有记忆门，但是记忆门的结构比较复杂，包括Input Gate（输入门，能不能往memory里写数据，由网络自己学），Output Gate（输出门，其他神经元能不能从memory中读数据，由网络自己学）和forget Gate（遗忘门，决定要不要把memory中的数据格式化，由网络自己学）。<br>简单来说，LSTM的Cell有四个输入：要写到memory的数据、三个门的开关信号；</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/xCGidAeyS4M?t=1180">https://youtu.be/xCGidAeyS4M?t=1180</a></p>
<p>LSTM运行的真实过程：<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1n67kx67sj218a0n276m.jpg" alt=""><br><a target="_blank" rel="noopener" href="https://youtu.be/xCGidAeyS4M?t=1682">https://youtu.be/xCGidAeyS4M?t=1682</a></p>
<ul>
<li>Self attention如何应用到图（Graph）中？</li>
</ul>
<p>1、直线的attention matrix是学出来的；但对于Graph来说，edge已经提供了连接关系；</p>
<p>2、所以，在计算attention matrix时，只计算有edge相连的node；</p>
<p>3、当把graph应用到self attention上时，其实就是一种 Graph neuro network；</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2580">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2580</a></p>
<p>4、那self-attention囊括了所有的Graph  Neuro Network吗？</p>
<p>5、GNN网络的详细介绍（图神经网络）：</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2593++">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2593++</a></p>
<p>图神经网络：在之前的课程中已经介绍过了</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA">https://youtu.be/eybCCtNKwzA</a></p>
<p>姜成翰助教来讲新的内容，</p>
<p>1、图神经网络的基本介绍。</p>
<p>Graph：图，包含节点和权重边；就是连在一起的结构；节点有节点的性质，边有边的性质；</p>
<p>Neural Network：</p>
<p>神经网络的入门级别：CNN -&gt; RNN -&gt; Transformer -&gt; </p>
<p>2、图神经网络能够解决什么问题？</p>
<p>问题：</p>
<p>我们为什么要用到图神经网络呢？（GNN）</p>
<p>我们怎么把一张图塞到一个Neural Network里呢？</p>
<p>图神经网络能够解决什么问题，做什么事情？</p>
<p>（1）用图神经网络，来开发新药。Train一个generator，（也就是一个decoder），它只吃一些随机的node（random node）</p>
<p>图神经网络的输入是一个graph</p>
<hr>
<p>日剧，忒休斯之船。找凶手的过程，可以看成是一个classification的问题；每个角色是一个entity，每个entity都有对应的attributes  如何train？</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qonlxb47j21840mfdj2.jpg" alt=""></p>
<p>角色和角色之间的关系，如何丢到模型中进行训练？</p>
<p>如何考虑所有entity之间的关系？这个时候就必须要用图神经网络了；</p>
<p>一个问题：我们手上有海量的数据，但是只有有限的labeling，该怎么办？<br>一个图上边有label node和unlabel node。<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qordlnf3j216s0htmyt.jpg" alt=""></p>
<p>图神经网络学的，是node represention （node feature）</p>
<p>基本原则，近朱者赤近墨者黑；</p>
<p>如何把邻居的信息，aggreate到当前节点中；</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qov0gictj213e0odq5f.jpg" alt=""></p>
<p>图神经的卷积；<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qowchjwzj216p0enjtg.jpg" alt=""></p>
<p>如何在图中进行卷积？<br>（1）基于空间的卷积：sum(NN4G)，mean(DCNN, DGC, GraphSAGE)，weighted sum(MoNET, GAT, GIN)， LSTM(GraphSAGE)， Max pooling(GraphSAGE)</p>
<p>（2）基于频率的卷积(ChebNet  -&gt;  GCN -&gt;  HyperGCN)；</p>
<p>最常用的Model有两个：GAT和GCN；</p>
<p>GNN网络一般用在哪些任务中：<br>（1）有监督的分类问题上；<br>（2）半监督的学习问题上；<br>（3）表达学习上，Graph InfoMax;<br>（4）Generation（生成）: GraphVAE, MolGAN, etc</p>
<p>图神经网络在NLP（自然语言处理问题）上的应用；</p>
<p>图神经网络中一些常见的dataset：<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qp3v24mlj212h0o7die.jpg" alt=""></p>
<p>Spatial Based GNN<br>在GNN中如何更新Feature中的Value，和CNN的基本思想类似，用邻居的值，进行卷积后，得到新的权重。在GNN中补交卷积，叫aggregate。</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qp7h07rbj21bi0p2q64.jpg" alt=""></p>
<p>如何把每个node的feature合并起来，成为一个graph的feature？在GNN中，这件事儿叫read-out</p>
<p>第一个GNN神经网络：NN4G（Neural Networks for Graph）</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpe9y33kj21d20rm42u.jpg" alt=""></p>
<p>如何更新权重？ ：邻居们 加起来，然后乘上权重，在加上input对应的值；</p>
<p>GNN可以有很多层，这么多层怎么readout呢？每一层的feature都加起来，然后经过一个transform后，再加起来</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=1585">https://youtu.be/eybCCtNKwzA?t=1585</a></p>
<p>第二个GNN神经网络：DCNN（Diffusion-Convolution Neural Network）</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpjb7jb3j21c10qywi7.jpg" alt=""></p>
<p>第一层：距离是1的节点全部加起来，然后取平均；<br>第二层：不是用第二层的feature来更新，还是用第一层的feature来更新，只不过距离不再是1，而是2；<br><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=1763">https://youtu.be/eybCCtNKwzA?t=1763</a></p>
<p>GNN中有K层，就能看到距离是k的邻居的内容；</p>
<p>每一层的特征，就成为一个矩阵；<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpp1t2ppj21bt0qvaf2.jpg" alt=""></p>
<p>如果需要做node的feature classification，就把所有层的的第一个节点的feature拿出来，拼成一个矩阵，然后 经过一个矩阵，得到一个值<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpqsc1yoj214u0lyta9.jpg" alt=""></p>
<p>还有一个网络叫：DGC（Diffusion Graph Convolution）<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpr2gik8j215w0ovmz6.jpg" alt=""></p>
<p>它是把所有层的feature map 加起来，再做判断；</p>
<p>另外一个model ： MoNET（Mixture Model Networks）</p>
<p>出发点：每个邻居都有不同的weight（权重不一样）</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=1878">https://youtu.be/eybCCtNKwzA?t=1878</a></p>
<p>该model，定义了节点到节点之间的距离（设计到了节点的度）：<br><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=1924">https://youtu.be/eybCCtNKwzA?t=1924</a></p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpvyei1dj21910pyaej.jpg" alt=""></p>
<p>另外一model： GraphSAGE<br><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=1991">https://youtu.be/eybCCtNKwzA?t=1991</a></p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qpx61bedj215d0otdiy.jpg" alt=""></p>
<p>在GraphSAGE网络中，有很多种来利用邻居进行aggregation：mean，max-poling, lstm.</p>
<p>其中最具特色的是把LSTM的思想，运用到图网络中；即，把邻居的feature喂到LSTM中，然后把它最后一层的hidden state当做是output，然后拿这个东西来update。我们知道LSTM是针对有顺序的data的，那么怎么处理GNN中一个节点的邻居们呢？这个时候就是随机搞一个顺序，丢到LSTM中。因为是随机的，所以GNN可能到最后，就会学会忽略顺序对结果的影响；</p>
<p>还有一个网络GAT（Graph Attention Networks）</p>
<p>特点是：不只要做weighted sum ，这个weight sum还要自己去学；</p>
<p>用一个节点和邻居节点，算出来energy的东西；energy衡量了邻居对当前这个节点的重要性。然后在下一层中，用能量energy 乘上 邻居的value，再求和，就得到下一层的的feature map。这种网络就叫 Graph Attention Networks。<br><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=2153">https://youtu.be/eybCCtNKwzA?t=2153</a></p>
<p>GIN（Graph Isomorphism Network） 图同构网络；</p>
<p>理论分析，为什么有的方法是work的，有的方法不可能work。为什么。这就是理论分析；</p>
<p>为什么不要用mean 和max pooling，而要用sum？  以为在前两者无法区分邻居都是一样的情况。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/eybCCtNKwzA?t=2254">https://youtu.be/eybCCtNKwzA?t=2254</a></p>
<p>还要加上节点本身的feature ，但是本身的权重可以去学。</p>
<hr>
<p>图神经网络的第二课；<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8">https://youtu.be/M9ht8vsVEw8</a></p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=43">https://youtu.be/M9ht8vsVEw8?t=43</a><br>Deep Graph Library中已经有很多写好的GNN</p>
<p>这一节主要讲的是，<br>（1）graph signal processing；<br>（2）Spectral-based graph neural network；</p>
<p>基本思想：<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qqy0181lj21770pdwhm.jpg" alt=""></p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=196">https://youtu.be/M9ht8vsVEw8?t=196</a></p>
<p>Spectral Graph Theory  谱图理论</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=420">https://youtu.be/M9ht8vsVEw8?t=420</a></p>
<p>需要明确两个概念：</p>
<p>邻接矩阵（adjacency matrix）：节点和节点之间是否相连？</p>
<p>度矩阵（degree matrix）：对角线上都是该节点有多少个邻居；</p>
<p>Graph Signal：每个节点上是一个信号，类似于真实的神经元信号；</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qr3o08ivj217n0oawg4.jpg" alt=""></p>
<p>Graph Laplacian（图的拉普拉斯变换）</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qr5kalcdj210v0pdacy.jpg" alt=""></p>
<p>这个课程中将的内容呢，每个节点都是一个标量；</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qr9yuafoj218s0pon0f.jpg" alt=""></p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=811">https://youtu.be/M9ht8vsVEw8?t=811</a></p>
<p>当把laplacian apply到一个graph上之后，相当于在某个信号跟他邻居的能量差。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=1148">https://youtu.be/M9ht8vsVEw8?t=1148</a></p>
<p>这个式子代表的是：power of signal variation between nodes. 代表两个节点之间的信号能量差；</p>
<p>带你直观地理解 图的频率：</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qrj3reeyj219y0ptgp5.jpg" alt=""></p>
<p>图的傅里叶变换：<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=1557">https://youtu.be/M9ht8vsVEw8?t=1557</a></p>
<p>图的傅里叶反变换：<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=1744">https://youtu.be/M9ht8vsVEw8?t=1744</a></p>
<p>讲这些内容的目标是为了，找到一个可以在图graph上做filter的东西。在频率上直接想乘，相当于在时域上做卷积。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=1923">https://youtu.be/M9ht8vsVEw8?t=1923</a></p>
<p>当前方法的局限性：<br><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qru0be7ij219m0phdiv.jpg" alt=""></p>
<p>模型要学的filter的size跟graph的size有关，这样带来的问题是，模型的学习能力跟输入图的大小尺寸相关。（模型的参数越多，说明模型的学习能力越强。）</p>
<p>Spectral Graph 网络存在的关键局限性是：它不是localize的；</p>
<p>一个GNN网络：ChebNet（属于Spectral Graph）</p>
<p>该网络的两个特点：第一，快；第二：localize；<br>他的学习能力是O(N)  不会随着模型的输入尺寸变化而变化。</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=2490">https://youtu.be/M9ht8vsVEw8?t=2490</a></p>
<p>如何解决时间成本高昂的问题？chebyshev polynomial</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=2637">https://youtu.be/M9ht8vsVEw8?t=2637</a></p>
<p>为什么要用chebyshev polynomial，意义是：通过这种方法，可以让参数变得非常好算。</p>
<p>前边没听懂没关系，这里是结论：<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=3087">https://youtu.be/M9ht8vsVEw8?t=3087</a></p>
<p>第二个网络GCN（最喜欢用的，比较直观，背后有数学基础）</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qscufurtj21780pa0vp.jpg" alt=""></p>
<p>GCN中feature map 更新的策略是：先让所有节点经过一个transform之后，所有的neighbor包起来，取平均。</p>
<p>把一张自然图像，转换成一张图：  </p>
<p>自然图像  ——&gt;super pixel（像素之间存在link）    </p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=3537">https://youtu.be/M9ht8vsVEw8?t=3537</a></p>
<p>一个公开数据集，通过观察分子的结构，预测分子的溶解度：<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=3579">https://youtu.be/M9ht8vsVEw8?t=3579</a></p>
<p>一个任务：辨别node属于哪些cluster？<br>一个任务：做edge的travel classification；</p>
<p>结论：GCN网络的效果并不是最好的；<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=3826">https://youtu.be/M9ht8vsVEw8?t=3826</a></p>
<p>可能的原因：<br>权重的指数decay</p>
<p>drop edge；</p>
<p>Graph Generation，的目标是：生成图片。<br>这种任务，传统的方法也能做：VAE-based model， GAN-based model，还有auto regressive based model；</p>
<p><img src="http://ww3.sinaimg.cn/large/a80ede49gy1h1qsqa3hb5j215v0p2775.jpg" alt=""></p>
<p>图神经网络的总结：<br><a target="_blank" rel="noopener" href="https://youtu.be/M9ht8vsVEw8?t=4151">https://youtu.be/M9ht8vsVEw8?t=4151</a></p>
<ul>
<li>self attention的各种变形；</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2616">https://youtu.be/gmsMY5kc-zw?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=2616</a></p>
<ul>
<li>self attention最大的问题，就是运算量非常之大；</li>
</ul>
<h4 id="【機器學習2021】Transformer-上"><a href="#【機器學習2021】Transformer-上" class="headerlink" title="【機器學習2021】Transformer (上)"></a>【機器學習2021】Transformer (上)</h4><ul>
<li>transformer和bert有什么关系？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=28">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=28</a></p>
<ul>
<li>transformer是什么？就是一个seq2seq的model</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=43">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=43</a></p>
<ul>
<li>硬train一发，能不能解决问题？</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=428">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=428</a></p>
<ul>
<li>台语的语音合成</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=568">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=568</a></p>
<ul>
<li>seq2seq的model训练一个聊天机器人；</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=675">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=675</a></p>
<ul>
<li>定制化的模型，往往比seq2seq更好；</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=913">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=913</a></p>
<ul>
<li>seq2seq用来进行语法解析；</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1198">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1198</a></p>
<ul>
<li>seq2seq用来解决多标签分类问题；（multi label classification）同一个目标，属于多一个类别</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1217">https://youtu.be/n9TlOhRjYoc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1217</a></p>
<ul>
<li>现在开始学，如何做seq2seq的model.</li>
</ul>
<p>【機器學習2021】Transformer (下)</p>
<p>【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (一) – 基本概念介紹</p>
<p>【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (二) – 理論介紹與WGAN</p>
<p>【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (三) – 生成器效能評估與條件式生成</p>
<p>【機器學習2021】生成式對抗網路 (Generative Adversarial Network, GAN) (四) – Cycle GAN</p>
<h3 id="【機器學習2021】自督導式學習-Self-supervised-Learning-一-–-芝麻街與進擊的巨人"><a href="#【機器學習2021】自督導式學習-Self-supervised-Learning-一-–-芝麻街與進擊的巨人" class="headerlink" title="【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人"></a>【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人</h3><p>自监督学习是无监督学习的一种。</p>
<p>【機器學習2021】自督導式學習 (Self-supervised Learning) (二) – BERT簡介</p>
<p>【機器學習2021】自督導式學習 (Self-supervised Learning) (三) – BERT的奇聞軼事</p>
<p>【機器學習2021】自督導式學習 (Self-supervised Learning) (四) – GPT的野望</p>
<h3 id="【機器學習2021】自編碼器-Auto-encoder-上-–-基本概念"><a href="#【機器學習2021】自編碼器-Auto-encoder-上-–-基本概念" class="headerlink" title="【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念"></a>【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念</h3><p>自编码器模型的encoder跟降维之间有联系。</p>
<p>李宏毅老师讲PCA和t-SNE</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=469">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=469</a></p>
<p>Auto-Encoder好在哪里呢？</p>
<p>为什么Auto-Encoder能够work？</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=632">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=632</a></p>
<p>encoder的事情就是化繁为简：</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=726">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=726</a></p>
<p>Hinton使用Restricted Boltzmann Machine（受限玻尔兹曼机）对不同层的auto encoder分开进行训练</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=818">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=818</a></p>
<p>为什么现在都没啥人用，Restricted Boltzmann Machine（受限玻尔兹曼机）了呢？</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=908">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=908</a></p>
<p>De-noising Auto-encoder去噪的自编码器</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=964">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=964</a></p>
<p>Bert的模型就是encoder，输出就是embedding，然后跟着一个linear模型做decoder</p>
<p><a target="_blank" rel="noopener" href="https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1085">https://youtu.be/3oHlf8-J3Nc?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&amp;t=1085</a></p>
<p>【機器學習2021】自編碼器 (Auto-encoder) (下) – 領結變聲器與更多應用</p>
<p>【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (上) – 基本概念</p>
<p>【機器學習2021】來自人類的惡意攻擊 (Adversarial Attack) (下) – 類神經網路能否躲過人類深不見底的惡意？</p>
<p>【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (上) – 為什麼類神經網路可以正確分辨寶可夢和數碼寶貝呢？</p>
<p>【機器學習2021】機器學習模型的可解釋性 (Explainable ML) (下) –機器心中的貓長什麼樣子？</p>
<p>【機器學習2021】概述領域自適應 (Domain Adaptation)</p>
<p>【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (一) – 增強式學習跟機器學習一樣都是三個步驟</p>
<p>【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (二) – Policy Gradient 與修課心情</p>
<p>【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (三) - Actor-Critic</p>
<p>【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (四) - 回饋非常罕見的時候怎麼辦？機器的望梅止渴</p>
<p>【機器學習2021】概述增強式學習 (Reinforcement Learning, RL) (五) - 如何從示範中學習？逆向增強式學習 (Inverse RL)</p>
<p>【機器學習2021】機器終身學習 (Life Long Learning, LL) (一) - 為什麼今日的人工智慧無法成為天網？災難性遺忘(Catastrophic Forgetting)</p>
<p>【機器學習2021】機器終身學習 (Life Long Learning, LL) (二) - 災難性遺忘(Catastrophic Forgetting)的克服之道</p>
<p>【機器學習2021】神經網路壓縮 (Network Compression) (一) - 類神經網路剪枝 (Pruning) 與大樂透假說 (Lottery Ticket Hypothesis)</p>
<p>【機器學習2021】神經網路壓縮 (Network Compression) (二) - 從各種不同的面向來壓縮神經網路</p>
<p>【機器學習2021】元學習 Meta Learning (一) - 元學習跟機器學習一樣也是三個步驟</p>
<p>【機器學習2021】元學習 Meta Learning (二) - 萬物皆可 Meta</p>
<p>【機器學習2021】課程結語 - 最後的業配並改編《為學一首示子姪》作結</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"># 笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/03/06/%E8%AF%BB%E3%80%8A%E5%AF%8C%E5%85%B0%E5%85%8B%E6%9E%97%E8%87%AA%E4%BC%A0%E3%80%8B/" rel="prev" title="读《富兰克林自传》">
      <i class="fa fa-chevron-left"></i> 读《富兰克林自传》
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/08/25/%E8%AF%BB%E3%80%8A%E6%95%B0%E5%AD%A6%E4%B9%8B%E7%BE%8E%E3%80%8B/" rel="next" title="读《数学之美》">
      读《数学之美》 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E9%A0%90%E6%B8%AC%E6%9C%AC%E9%A0%BB%E9%81%93%E8%A7%80%E7%9C%8B%E4%BA%BA%E6%95%B8-%E4%B8%8A-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%B0%A1%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">【機器學習2021】預測本頻道觀看人數 (上) - 機器學習基本概念簡介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E9%A0%90%E6%B8%AC%E6%9C%AC%E9%A0%BB%E9%81%93%E8%A7%80%E7%9C%8B%E4%BA%BA%E6%95%B8-%E4%B8%8B-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E7%B0%A1%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">【機器學習2021】預測本頻道觀看人數 (下) - 深度學習基本概念簡介</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%E8%A8%93%E7%B7%B4%E4%B8%8D%E8%B5%B7%E4%BE%86%E6%80%8E%E9%BA%BC%E8%BE%A6-%E4%B8%80-%EF%BC%9A-%E5%B1%80%E9%83%A8%E6%9C%80%E5%B0%8F%E5%80%BC-local-minima-%E8%88%87%E9%9E%8D%E9%BB%9E-saddle-point"><span class="nav-number">3.</span> <span class="nav-text">【機器學習2021】類神經網路訓練不起來怎麼辦 (一)： 局部最小值 (local minima) 與鞍點 (saddle point)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-Convolutional-Neural-Networks-CNN"><span class="nav-number">4.</span> <span class="nav-text">【機器學習2021】卷積神經網路 (Convolutional Neural Networks, CNN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-Self-attention-%E4%B8%8A"><span class="nav-number">5.</span> <span class="nav-text">【機器學習2021】自注意力機制 (Self-attention) (上)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A9%9F%E5%88%B6-Self-attention-%E4%B8%8B"><span class="nav-number">6.</span> <span class="nav-text">【機器學習2021】自注意力機制 (Self-attention) (下)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91Transformer-%E4%B8%8A"><span class="nav-number">7.</span> <span class="nav-text">【機器學習2021】Transformer (上)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E8%87%AA%E7%9D%A3%E5%B0%8E%E5%BC%8F%E5%AD%B8%E7%BF%92-Self-supervised-Learning-%E4%B8%80-%E2%80%93-%E8%8A%9D%E9%BA%BB%E8%A1%97%E8%88%87%E9%80%B2%E6%93%8A%E7%9A%84%E5%B7%A8%E4%BA%BA"><span class="nav-number"></span> <span class="nav-text">【機器學習2021】自督導式學習 (Self-supervised Learning) (一) – 芝麻街與進擊的巨人</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E3%80%90%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%922021%E3%80%91%E8%87%AA%E7%B7%A8%E7%A2%BC%E5%99%A8-Auto-encoder-%E4%B8%8A-%E2%80%93-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number"></span> <span class="nav-text">【機器學習2021】自編碼器 (Auto-encoder) (上) – 基本概念</span></a></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Feng Zhiheng"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Feng Zhiheng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Feng Zhiheng</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">239k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:38</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
